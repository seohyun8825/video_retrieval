#!/usr/bin/env bash

set -euo pipefail

# Push a trained ECVA model checkpoint to HF with a README and inference example.
# - Assumes huggingface-cli login is already done.
# - Uploads the whole model dir (configs included) plus autogenerated README.md and inference_example.py.
#
# Usage:
#   t2_model_push_with_Readme.bash [MODEL_DIR]
#
# Config (env-overridable):
#   HF_USER   : HF username/org (auto from whoami if empty)
#   HF_REPO   : Repo name (default: ecva-sft-YYYYMMDD)
#   HF_PRIVATE: true/false (default: false)
#   HF_BRANCH : target branch (default: main)
#   COMMIT_MESSAGE: commit message (default: "Add ${HF_REPO}")

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
ROOT_DIR="$(cd "${SCRIPT_DIR}/../../.." && pwd)"

MODEL_DIR="${1:-${MODEL_DIR:-/hub_data4/seohyun/saves/ecva_instruct/full/sft/checkpoint-350}}"

HF_USER="${HF_USER:-}"
HF_REPO="${HF_REPO:-}"
HF_PRIVATE="${HF_PRIVATE:-false}"
HF_BRANCH="${HF_BRANCH:-main}"
COMMIT_MESSAGE="${COMMIT_MESSAGE:-}"

if [[ -z "${HF_REPO}" ]]; then
  HF_REPO="ecva-sft-$(date +%Y%m%d)"
fi
if [[ -z "${COMMIT_MESSAGE}" ]]; then
  COMMIT_MESSAGE="Add ${HF_REPO}"
fi

# Resolve HF_USER
if [[ -z "${HF_USER}" ]]; then
  set +e
  HF_USER=$(python3 - << 'PY'
try:
    from huggingface_hub import HfApi, HfFolder
    tok = HfFolder.get_token()
    if not tok:
        raise SystemExit(1)
    info = HfApi().whoami(tok)
    print(info.get("name",""))
except Exception:
    raise SystemExit(2)
PY
  )
  set -e
  if [[ -z "${HF_USER}" && -x "$(command -v huggingface-cli)" ]]; then
    HF_USER=$(huggingface-cli whoami -s 2>/dev/null | head -n1 || true)
  fi
fi

sanitize() {
  local s="$1"
  s="$(printf %s "$s" | tr -cd 'A-Za-z0-9._-')"
  s="$(printf %s "$s" | sed -E 's/^[-.]+//; s/[-.]+$//')"
  printf %s "$s"
}

HF_USER_ORIG="${HF_USER}"
HF_USER="$(sanitize "${HF_USER}")"
HF_REPO_ORIG="${HF_REPO}"
HF_REPO="$(sanitize "${HF_REPO}")"
if [[ -z "${HF_USER}" || -z "${HF_REPO}" ]]; then
  echo "HF_USER/HF_REPO invalid. HF_USER='${HF_USER_ORIG}' HF_REPO='${HF_REPO_ORIG}'" >&2
  exit 1
fi

REPO_ID="${HF_USER}/${HF_REPO}"

if [[ ! -d "${MODEL_DIR}" ]]; then
  echo "MODEL_DIR not found: ${MODEL_DIR}" >&2
  exit 1
fi

echo "Model dir : ${MODEL_DIR}"
echo "Repo id   : ${REPO_ID} (model)"
echo "Branch    : ${HF_BRANCH}"
echo "Private   : ${HF_PRIVATE}"

tmp_dir="$(mktemp -d)"
trap 'rm -rf "${tmp_dir}"' EXIT

# Build README
cat > "${tmp_dir}/README.md" <<'EOF'
# ECVA Instruct (SFT checkpoint)

- Modality: Video + text (Qwen-VL style prompt)
- Example base: Qwen/Qwen2-VL (fine-tuned on ECVA anomalies)

## Quick inference (local VLLM server)

1) Start a vLLM OpenAI server:
```bash
pip install vllm decord transformers
python -m vllm.entrypoints.openai.api_server \
  --model <REPO_ID> \
  --trust-remote-code \
  --tensor-parallel-size 1 \
  --port 8099
```

2) Send a prompt + video (OpenAI client):
```python
from openai import OpenAI
client = OpenAI(base_url="http://localhost:8099/v1", api_key="EMPTY")

video_path = "/path/to/video.mp4"
prompt = "Are any anomalies directly occurring in this clip? If yes, identify them briefly."

with open(video_path, "rb") as f:
    video_bytes = f.read()

resp = client.chat.completions.create(
    model="<REPO_ID>",
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": prompt},
                {"type": "input_video", "video": {"data": video_bytes, "mime_type": "video/mp4"}},
            ],
        }
    ],
    max_tokens=1024,
    temperature=0.2,
)
print(resp.choices[0].message.content)
```

## Files
- Full checkpoint directory (includes configs).
- This README and `inference_example.py` for convenience.
EOF
sed -i "s|<REPO_ID>|${REPO_ID}|g" "${tmp_dir}/README.md"

# Build inference_example.py
cat > "${tmp_dir}/inference_example.py" <<'EOF'
#!/usr/bin/env python3
import argparse, base64, os
from openai import OpenAI

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--api_base", default="http://localhost:8099/v1")
    ap.add_argument("--api_key", default="EMPTY")
    ap.add_argument("--model", required=True, help="HF repo id or model name served by vLLM")
    ap.add_argument("--video", required=True, help="Path to an mp4/mkv video")
    ap.add_argument("--prompt", default="Are any anomalies directly occurring in this clip? If yes, identify them briefly.")
    ap.add_argument("--temperature", type=float, default=0.2)
    ap.add_argument("--max_tokens", type=int, default=512)
    args = ap.parse_args()

    client = OpenAI(base_url=args.api_base, api_key=args.api_key)
    with open(args.video, "rb") as f:
        video_bytes = f.read()

    resp = client.chat.completions.create(
        model=args.model,
        messages=[
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": args.prompt},
                    {"type": "input_video", "video": {"data": video_bytes, "mime_type": "video/mp4"}},
                ],
            }
        ],
        temperature=args.temperature,
        max_tokens=args.max_tokens,
    )
    print(resp.choices[0].message.content)

if __name__ == "__main__":
    main()
EOF

create_flags=(--type model --exist-ok)
if [[ "${HF_PRIVATE,,}" == "true" ]]; then
  create_flags+=(--private)
fi

huggingface-cli repo create "${REPO_ID}" "${create_flags[@]}" >/dev/null

# Upload model dir (all files/configs)
huggingface-cli upload "${REPO_ID}" "${MODEL_DIR}" "." \
  --repo-type model \
  --commit-message "${COMMIT_MESSAGE}" \
  --revision "${HF_BRANCH}"

# Upload README + example
huggingface-cli upload "${REPO_ID}" "${tmp_dir}/README.md" README.md \
  --repo-type model \
  --commit-message "Add README" \
  --revision "${HF_BRANCH}"
huggingface-cli upload "${REPO_ID}" "${tmp_dir}/inference_example.py" inference_example.py \
  --repo-type model \
  --commit-message "Add inference example" \
  --revision "${HF_BRANCH}"

echo "âœ… Pushed to https://huggingface.co/${REPO_ID}/tree/${HF_BRANCH}"
